# tensorflow初体验之房价预测

了解了 tensorflow 的基本概念后，接下来，我们将以一个 房价预测 的案例为例，演示 tensorflow 的基本功能使用。

## 背景知识说明

开始正式进入 Tensorflow 实战之前，我们还需要了解一些机器学习相关的背景知识。

### 监督学习（supervised learning）

监督学习是机器学习的方法之一，它是指从训练数据（输入和预期输出）中学到的一个模型（函数），并且可以根据模型推断出新实例的方法。

其中，函数的输出既可以是一个连续值（如回归分析）或一个离散值/类别标签（如分类问题）。

![supervised_learning](./pictures/supervised_learning.png)

上图表示了一个监督学习的基本流程。

典型的监督学习算法非常多，例如：

 - 线性回归（Linear Regression）
 - 逻辑回归（Logistic Regression）
 - 决策树（Decision Tree）
 - 随机森林（Random Forest）
 - 最近邻算法（k-NN）
 - 朴素贝叶斯（Naive Bayes）
 - 支持向量机（SVM）
 - 感知器（Perceptron）
 - 深度神经网络（DNN）


### 线性回归与梯度下降法

线性回归可以说是监督学习中最简单的算法了，下面我们来对线性回归进行分析。

在统计学中，线性回归是指 **利用线性回归方程的最小二乘函数对一个或多个自变量和因变量之间的关系进行建模的一种回归分析** 。

这种函数是一个或多个称为回归系数的模型参数的线性组合。

以 单变量线性回归 为例，如果一个模型是线性关系的，那么它可以表示如下：

$$
y = wx + b
$$

那么，我们也就可以假设函数如下：

$$
h_\theta(x) = \theta(x)_0 + \theta(x)_{1}x_1 = \theta(x)_{0}x_0 + \theta(x)_{1}x_1 = \theta(x)^Tx  (x_0=1)
$$

其中，$$\theta$$ 就是我们假设的函数参数。

而假设函数和理想模型的损失值（误差）就是：

$$
loss = y - h_{\theta}(x)
$$

因此，我们想要做的就是从一组样本 $$(x_i, y_i)$$ 中找出误差最小的 $$\theta$$ 值。
此时，我们可以使用最小二乘法，即它的优化目标为最小化残差平方和：

$$
J(\theta) = \frag{1}{n} \sum_{i=1}^{n} (h_{\theta}(x_i) - y_i)^2
$$

而梯度迭代法就是指在优化目标函数的每一轮迭代中，都按照模型参数 $$\theta$$ 的梯度方向进行变更，即表达式如下：

$$
\theta_j := \theta_j - \alpha \frag{\partial}{\partial\theta_j} J(\theta)
$$

代入 $$J(\theta)$$ 进行求导，得到的结果如下：

$$
\theta_j := \theta_j - 2\alpha \frag{1}{n} \sum_{i=1}^{n} (h_{\theta}(x_i) - y_i) (x_i)
$$

下面，我们来看一下 **多变量** 线性回归的场景：

多变量线性回归可以表示如下：

$$
y = w_{0} + w_{1}x_{1} + w_{2}x_{2} = W^{T}X
$$

那么，我们也就可以假设函数如下：

$$
h_\theta(X) = \theta(x)_0 + \theta(x)_{1}x_1 + \theta(x)_{2}x_2 = \theta(x)^{T}X  (x_0=1)
$$

其中，$$\theta$$ 就是我们假设的函数参数。

而假设函数和理想模型的损失值（误差）就是：

$$
loss = y - h_{\theta}(X)
$$

同样可以使用最小二乘法，即它的优化目标为最小化残差平方和：

$$
J(\theta) = (h_\theta(X) - y)^T(h_\theta(X) - y)
$$


## 问题描述



## 数据读入



## 数据预处理与可视化



## 模型训练


